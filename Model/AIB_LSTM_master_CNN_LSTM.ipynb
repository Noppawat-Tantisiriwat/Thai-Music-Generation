{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIB_LSTM_master_CNN_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "p_0BSCBeBh2c",
        "l6242cVZrNP-"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noppawat-Tantisiriwat/Thai-Music-Generation/blob/main/AIB_LSTM_master_CNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fxEWI_WFncQ"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASh9TLpFvwG"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, LayerNormalization, \\\n",
        "    Flatten, Dense, Reshape, Conv1DTranspose, Layer, LSTM, RepeatVector, TimeDistributed\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import datetime, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-VmvLF8FnPO"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXWh3xT3GTvb",
        "outputId": "df606215-1a6d-4d69-851f-43f5541608b9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jun 23 00:43:33 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp8nuEHGZp4L"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_Onm4L0FGnl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIkvHTC083lr"
      },
      "source": [
        "class Encoder(Model):\n",
        "  def __init__(self, \n",
        "               inp_shape: List[int],\n",
        "               conv_filters: List[int],\n",
        "               conv_kernels: List[int],\n",
        "               conv_strides: List[int],\n",
        "               latent_space_dim: int,\n",
        "               lstm_units : int, \n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.conv_filters = conv_filters # [2, 4, 8]\n",
        "    self.conv_kernels = conv_kernels # [3, 5, 3]\n",
        "    self.conv_strides = conv_strides # [1, 2, 2]\n",
        "    self.latent_space_dim = latent_space_dim # 2\n",
        "    self._shape_before_bottleneck = None\n",
        "    self.lstm_units = lstm_units\n",
        "    # dim assertion\n",
        "    assert len(self.conv_strides) == len(self.conv_kernels) == len(self.conv_filters)\n",
        "\n",
        "    self.convs = [Conv1D(\n",
        "        filters=f,\n",
        "        kernel_size=k,\n",
        "        strides=s,\n",
        "        padding=\"same\",\n",
        "        name=f\"encoder_conv_layer_{i}\",\n",
        "        activation=\"relu\"\n",
        "    ) for i, (f, k, s) in enumerate(zip(self.conv_filters, self.conv_kernels, self.conv_strides))]\n",
        "\n",
        "    self.layernorms = [LayerNormalization(name=f\"encoder_ln_{i}\") for i in range(len(self.conv_filters))]\n",
        "    \n",
        "  \n",
        "    \n",
        "    # self.flatten = Flatten()\n",
        "    # dim assertion\n",
        "    assert len(self.convs) == len(self.layernorms)\n",
        "\n",
        "    self.dense_mu = Dense(self.latent_space_dim, name=\"mu\")\n",
        "\n",
        "    self.dense_logvar = Dense(self.latent_space_dim, name=\"log_variance\")\n",
        "\n",
        "    self._compute_shape_before_bottleneck(inp_shape)\n",
        "    self.lstm = LSTM(lstm_units, name=\"lstm\")\n",
        "\n",
        "  def _compute_shape_before_bottleneck(self, inp_shape: List[int]):\n",
        "    x = tf.zeros(shape=inp_shape) # dummy data\n",
        "    x= tf.expand_dims(x, axis=0) # batching\n",
        "    for conv, layernorm in zip(self.convs, self.layernorms):\n",
        "      x = conv(x)\n",
        "      x = layernorm(x) # flatten ด้วย LSTM \n",
        "    self._shape_before_bottleneck = tf.shape(x)[1:] # (None, shape) -> (shape) [None = batch_size]\n",
        "  \n",
        "  def _reparameterized(self, mu, log_var):\n",
        "    eps = K.random_normal(shape=K.shape(mu), mean=0., stddev=1.)\n",
        "    sample_point = mu + K.exp(log_var / 2) * eps\n",
        "    return sample_point\n",
        "\n",
        "  def call(self, x):\n",
        "    for conv, layernorm in zip(self.convs, self.layernorms):\n",
        "      x = conv(x)\n",
        "      x = layernorm(x)\n",
        "    x = self.lstm(x)\n",
        "    mu = self.dense_mu(x)\n",
        "    log_var = self.dense_logvar(x)\n",
        "    x = self._reparameterized(mu, log_var)\n",
        "    return x, (mu, log_var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcp1P4eXFF8w"
      },
      "source": [
        "class Decoder(Model):\n",
        "  def __init__(self,\n",
        "               shape_before_bottleneck: tf.Tensor,\n",
        "               conv_filters: List[int], # the first element must be 1\n",
        "               conv_kernels: List[int],\n",
        "               conv_strides: List[int],\n",
        "               out_channel: int,\n",
        "               **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.conv_filters = conv_filters\n",
        "    self.conv_kernels = conv_kernels\n",
        "    self.conv_strides = conv_strides\n",
        "    self.reshape = Reshape(shape_before_bottleneck.numpy())\n",
        "    self.out_channel = out_channel\n",
        "    self.dense = Dense(tf.reduce_prod(shape_before_bottleneck))\n",
        "    # dim assertion\n",
        "    assert len(self.conv_strides) == len(self.conv_kernels) == len(self.conv_filters)\n",
        "\n",
        "    self.convs = [Conv1DTranspose(\n",
        "          filters=f,\n",
        "            kernel_size=k,\n",
        "            strides=s,\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{i}\",\n",
        "            activation=\"relu\"\n",
        "          ) for i, (f, k, s) in enumerate(zip(self.conv_filters[1:], self.conv_kernels[1:], self.conv_strides[1:]))]\n",
        "    self.layernorms = [LayerNormalization(name=f\"decoder_ln_{i}\") for i in range(len(self.conv_filters[1:]))]\n",
        "    # self.lstms = [LSTM(unit, return_sequences=True) for unit in self.conv_filters[1:]]\n",
        "    # dim assertion\n",
        "    assert len(self.convs) == len(self.layernorms)\n",
        "  \n",
        "    self.output_conv = Conv1DTranspose(\n",
        "        filters=self.out_channel,\n",
        "        kernel_size=self.conv_kernels[0],\n",
        "        strides=self.conv_strides[0],\n",
        "        padding=\"same\",\n",
        "        activation=\"sigmoid\",\n",
        "        name=f\"decoder_conv_transpose_layer_{len(self.conv_strides)}\"\n",
        "    )\n",
        "\n",
        "    \n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    x = self.reshape(x)\n",
        "    for conv,layernorm in zip(self.convs, self.layernorms):\n",
        "            x = conv(x)\n",
        "            x = layernorm(x)\n",
        "    x = self.output_conv(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y5YW1WdFQHR"
      },
      "source": [
        "class VAE(Model):\n",
        "  def __init__(self,\n",
        "               inp_shape: List[int],\n",
        "               conv_filters: List[int],\n",
        "               conv_kernels: List[int],\n",
        "               conv_strides: List[int],\n",
        "               latent_space_dim: int,\n",
        "               lstm_units : int,\n",
        "               recon_loss_weight: int,\n",
        "               **kwargs):\n",
        "    super(VAE, self).__init__(**kwargs)\n",
        "    self.inp_shape = inp_shape\n",
        "    self.recon_loss_weight = recon_loss_weight \n",
        "    self._shape_before_bottleneck = None\n",
        "    self.latent_space_dim = latent_space_dim\n",
        "    self._reduce_axis = list(range(1, len(inp_shape)+1))\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "        inp_shape=inp_shape,\n",
        "        conv_filters=conv_filters,\n",
        "        conv_kernels=conv_kernels,\n",
        "        conv_strides=conv_strides,\n",
        "        latent_space_dim=latent_space_dim,\n",
        "        lstm_units = lstm_units\n",
        "    )\n",
        "    \n",
        "    self.decoder = Decoder(\n",
        "        shape_before_bottleneck=self.encoder._shape_before_bottleneck,\n",
        "        conv_filters = conv_filters[::-1],\n",
        "        conv_kernels = conv_kernels[::-1],\n",
        "        conv_strides=conv_strides[::-1],\n",
        "        out_channel=inp_shape[-1]\n",
        "    )\n",
        "\n",
        "  def _calculate_kl_loss(self, mu, log_var):\n",
        "    kl_loss = -0.5 * tf.reduce_sum(1 + log_var -tf.square(mu) - tf.exp(log_var), axis=1)\n",
        "    return kl_loss\n",
        "\n",
        "  def _calculate_recon_loss(self, x, x_prime):\n",
        "    recon_loss = tf.reduce_mean(tf.square(x - x_prime), axis=self._reduce_axis)\n",
        "    return self.recon_loss_weight * recon_loss\n",
        "  \n",
        "  def _compute_loss(self, x, x_prime, mu, log_var):\n",
        "    recon_loss =  self._calculate_recon_loss(x, x_prime)\n",
        "    kl_loss =  self._calculate_kl_loss(mu, log_var)\n",
        "    loss =  recon_loss  + kl_loss\n",
        "    self.add_loss(tf.add_n([loss]))\n",
        "    self.add_metric(tf.add_n([recon_loss / self.recon_loss_weight]), name=\"recon_loss\")\n",
        "    self.add_metric(tf.add_n([kl_loss]), name=\"kl_loss\")\n",
        "\n",
        "  def call(self, x):\n",
        "    z, (mu, log_var) = self.encoder(x)\n",
        "    x_prime = self.decoder(z)\n",
        "    self._compute_loss(x, x_prime, mu, log_var)\n",
        "    return z, x_prime\n",
        "\n",
        "  def full_summary(self):\n",
        "    self.encoder.summary()\n",
        "    self.decoder.summary()\n",
        "    self.summary()\n",
        "\n",
        "  def sample(self, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal([1, self.latent_space_dim])\n",
        "      return self.decoder(eps)\n",
        "    else:\n",
        "      print(f\"sample epsilon: {eps}\")\n",
        "      return self.decoder(eps)\n",
        "\n",
        "  def reconstruct(self, images):\n",
        "    latent_representations = self.encoder.predict(images)\n",
        "    reconstructed_images = self.decoder.predict(latent_representations)\n",
        "    return reconstructed_images, latent_representations\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00E6lTB9HMmr"
      },
      "source": [
        "# Training Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQEsAMCsKHt5"
      },
      "source": [
        "vae = VAE(inp_shape=[1296, 256], \n",
        "          conv_filters=[256, 512, 512, 1024],\n",
        "          conv_kernels=[5, 5, 5, 5],\n",
        "          conv_strides=[3, 3, 3, 3],\n",
        "          lstm_units=512,\n",
        "          latent_space_dim=1024,\n",
        "          recon_loss_weight=1000000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea1KEywlKSJi"
      },
      "source": [
        "_ = vae(Input(shape=[1296, 256]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GIU8AOwxRht"
      },
      "source": [
        "vae.compile(Adam(learning_rate=1e-4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwthi-9XdZhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8ae81f-2471-446f-ac85-d913c4e1a905"
      },
      "source": [
        "vae.full_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_conv_layer_0 (Conv1D multiple                  327936    \n",
            "_________________________________________________________________\n",
            "encoder_conv_layer_1 (Conv1D multiple                  655872    \n",
            "_________________________________________________________________\n",
            "encoder_conv_layer_2 (Conv1D multiple                  1311232   \n",
            "_________________________________________________________________\n",
            "encoder_conv_layer_3 (Conv1D multiple                  2622464   \n",
            "_________________________________________________________________\n",
            "encoder_ln_0 (LayerNormaliza multiple                  512       \n",
            "_________________________________________________________________\n",
            "encoder_ln_1 (LayerNormaliza multiple                  1024      \n",
            "_________________________________________________________________\n",
            "encoder_ln_2 (LayerNormaliza multiple                  1024      \n",
            "_________________________________________________________________\n",
            "encoder_ln_3 (LayerNormaliza multiple                  2048      \n",
            "_________________________________________________________________\n",
            "mu (Dense)                   multiple                  525312    \n",
            "_________________________________________________________________\n",
            "log_variance (Dense)         multiple                  525312    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  multiple                  3147776   \n",
            "=================================================================\n",
            "Total params: 9,120,512\n",
            "Trainable params: 9,120,512\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  16793600  \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer multiple                  2621952   \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer multiple                  1311232   \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer multiple                  655616    \n",
            "_________________________________________________________________\n",
            "decoder_ln_0 (LayerNormaliza multiple                  1024      \n",
            "_________________________________________________________________\n",
            "decoder_ln_1 (LayerNormaliza multiple                  1024      \n",
            "_________________________________________________________________\n",
            "decoder_ln_2 (LayerNormaliza multiple                  512       \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer multiple                  327936    \n",
            "=================================================================\n",
            "Total params: 21,712,896\n",
            "Trainable params: 21,712,896\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"vae\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder (Encoder)            multiple                  9120512   \n",
            "_________________________________________________________________\n",
            "decoder (Decoder)            multiple                  21712896  \n",
            "=================================================================\n",
            "Total params: 30,833,408\n",
            "Trainable params: 30,833,408\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj3bRSApxMG9"
      },
      "source": [
        "# Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_MGS43wy4HA"
      },
      "source": [
        "import librosa \n",
        "import soundfile as sf\n",
        "import IPython.display as ipd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvDPqs03j0Sz"
      },
      "source": [
        "import pickle as p\n",
        "with open(\"/content/drive/MyDrive/AIB_project/Attemps/VAE_code/MinMaxValue_padded/min_max_values.pkl\", \"rb\") as file:\n",
        "  max_min = p.load(file)\n",
        "min_li = []\n",
        "max_li = []\n",
        "for _, value in max_min.items():\n",
        "    max_li.append(value[\"max\"])\n",
        "    min_li.append(value[\"min\"])\n",
        "min_array = np.array(min_li)\n",
        "max_array = np.array(max_li)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfDPu1NqWra"
      },
      "source": [
        "original_min = min_array.mean()\n",
        "original_max = max_array.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL8xsaKhi6y5"
      },
      "source": [
        "def denormalise(norm_array, original_min, original_max):\n",
        "    array = (norm_array - 0.) / (1. - 0.)\n",
        "    array = array * (original_max - original_min) + original_min\n",
        "    return array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgLnNxATyYls"
      },
      "source": [
        "def convert_spectrograms_to_audio(spectrogram):\n",
        "# reshape the log spectro\n",
        "  log_spectrogram = tf.squeeze(spectrogram).numpy().T\n",
        "  log_spectrogram = denormalise(log_spectrogram, original_min, original_max)\n",
        "            # apply denormalisation\n",
        "            # log spectrogram -> spectrogram\n",
        "  spec = librosa.db_to_amplitude(log_spectrogram)\n",
        "            # apply Griffin-Lim\n",
        "  signal = librosa.griffinlim(spec, hop_length=256, win_length=510)\n",
        "  return signal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTkwr9eq0g6O"
      },
      "source": [
        "def generate(esp=None):\n",
        "  if eps == None:\n",
        "    esp = tf.random.normal([1, vae.encoder.latent_space_dim])\n",
        "    spectrogram_random = loaded_vae.decoder(eps)\n",
        "    wavs = convert_spectrograms_to_audio(spectrogram_random)\n",
        "    return wavs\n",
        "  else:\n",
        "    spectrogram_random = loaded_vae.decoder(eps)\n",
        "    wavs = convert_spectrograms_to_audio(spectrogram_random)\n",
        "    return wavs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx9GRhS9rWHI"
      },
      "source": [
        "def plot_spectrogram(spectrogram_random):\n",
        "  librosa.display.specshow(spectrogram_random.numpy()[0].T)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}