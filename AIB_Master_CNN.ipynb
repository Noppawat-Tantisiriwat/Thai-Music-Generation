{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AIB_Master_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "p_0BSCBeBh2c",
        "l6242cVZrNP-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noppawat-Tantisiriwat/Thai-Music-Generation/blob/main/AIB_Master_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fxEWI_WFncQ"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASh9TLpFvwG"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, LayerNormalization, \\\n",
        "    Flatten, Dense, Reshape, Conv1DTranspose, Layer\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import datetime, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-VmvLF8FnPO"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXWh3xT3GTvb",
        "outputId": "25d1391c-bf82-42a6-cef8-c1cfef5a55f6"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jun  9 08:55:30 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp8nuEHGZp4L"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfYEJXU5GWNc",
        "outputId": "76c933db-6470-47e3-adb0-5ec69307cef2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u5BVgUrGWu2",
        "outputId": "4bebb448-0fc8-4d75-e096-7cb242a42a22"
      },
      "source": [
        "%cd /content/drive/MyDrive/AIB_project/Attemps/VAE_experimental"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/AIB_project/Attemps/VAE_experimental\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_Onm4L0FGnl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIkvHTC083lr"
      },
      "source": [
        "class Encoder(Model):\n",
        "  def __init__(self, \n",
        "               inp_shape: List[int],\n",
        "               conv_filters: List[int],\n",
        "               conv_kernels: List[int],\n",
        "               conv_strides: List[int],\n",
        "               latent_space_dim: int, \n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.conv_filters = conv_filters # [2, 4, 8]\n",
        "    self.conv_kernels = conv_kernels # [3, 5, 3]\n",
        "    self.conv_strides = conv_strides # [1, 2, 2]\n",
        "    self.latent_space_dim = latent_space_dim # 2\n",
        "    self._shape_before_bottleneck = None\n",
        "    # dim assertion\n",
        "    assert len(self.conv_strides) == len(self.conv_kernels) == len(self.conv_filters)\n",
        "\n",
        "    self.convs = [Conv1D(\n",
        "        filters=f,\n",
        "        kernel_size=k,\n",
        "        strides=s,\n",
        "        padding=\"same\",\n",
        "        name=f\"encoder_conv_layer_{i}\",\n",
        "        activation=\"relu\"\n",
        "    ) for i, (f, k, s) in enumerate(zip(self.conv_filters, self.conv_kernels, self.conv_strides))]\n",
        "\n",
        "    self.layernorms = [LayerNormalization(name=f\"encoder_ln_{i}\") for i in range(len(self.conv_filters))]\n",
        "    \n",
        "    self.flatten = Flatten()\n",
        "    # dim assertion\n",
        "    assert len(self.convs) == len(self.layernorms)\n",
        "\n",
        "    self.dense_mu = Dense(self.latent_space_dim, name=\"mu\")\n",
        "\n",
        "    self.dense_logvar = Dense(self.latent_space_dim, name=\"log_variance\")\n",
        "\n",
        "    self._compute_shape_before_bottleneck(inp_shape)\n",
        "\n",
        "\n",
        "  def _compute_shape_before_bottleneck(self, inp_shape: List[int]):\n",
        "    x = tf.zeros(shape=inp_shape) # dummy data\n",
        "    x= tf.expand_dims(x, axis=0) # batching\n",
        "    for conv, layernorm in zip(self.convs, self.layernorms):\n",
        "      x = conv(x)\n",
        "      x = layernorm(x)\n",
        "    self._shape_before_bottleneck = tf.shape(x)[1:] # (None, shape) -> (shape) [None = batch_size]\n",
        "  \n",
        "  def _reparameterized(self, mu, log_var):\n",
        "    eps = K.random_normal(shape=K.shape(mu), mean=0., stddev=1.)\n",
        "    sample_point = mu + K.exp(log_var / 2) * eps\n",
        "    return sample_point\n",
        "\n",
        "  def call(self, x):\n",
        "    for conv, layernorm in zip(self.convs, self.layernorms):\n",
        "      x = conv(x)\n",
        "      x = layernorm(x)\n",
        "    x = self.flatten(x)\n",
        "    mu = self.dense_mu(x)\n",
        "    log_var = self.dense_logvar(x)\n",
        "    x = self._reparameterized(mu, log_var)\n",
        "    return x, (mu, log_var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcp1P4eXFF8w"
      },
      "source": [
        "class Decoder(Model):\n",
        "  def __init__(self,\n",
        "               shape_before_bottleneck: tf.Tensor,\n",
        "               conv_filters: List[int], # the first element must be 1\n",
        "               conv_kernels: List[int],\n",
        "               conv_strides: List[int],\n",
        "               out_channel: int,\n",
        "               **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.conv_filters = conv_filters\n",
        "    self.conv_kernels = conv_kernels\n",
        "    self.conv_strides = conv_strides\n",
        "    self.dense = Dense(tf.reduce_prod(shape_before_bottleneck), name=\"decoder_dense\")\n",
        "    self.reshape = Reshape(shape_before_bottleneck.numpy())\n",
        "    self.out_channel = out_channel\n",
        "\n",
        "    # dim assertion\n",
        "    assert len(self.conv_strides) == len(self.conv_kernels) == len(self.conv_filters)\n",
        "\n",
        "    self.convs = [Conv1DTranspose(\n",
        "          filters=f,\n",
        "            kernel_size=k,\n",
        "            strides=s,\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{i}\",\n",
        "            activation=\"relu\"\n",
        "          ) for i, (f, k, s) in enumerate(zip(self.conv_filters[1:], self.conv_kernels[1:], self.conv_strides[1:]))]\n",
        "    self.layernorms = [LayerNormalization(name=f\"decoder_ln_{i}\") for i in range(len(self.conv_filters[1:]))]\n",
        "\n",
        "    # dim assertion\n",
        "    assert len(self.convs) == len(self.layernorms)\n",
        "\n",
        "    self.output_conv = Conv1DTranspose(\n",
        "        filters=self.out_channel,\n",
        "        kernel_size=self.conv_kernels[0],\n",
        "        strides=self.conv_strides[0],\n",
        "        padding=\"same\",\n",
        "        activation=\"sigmoid\",\n",
        "        name=f\"decoder_conv_transpose_layer_{len(self.conv_strides)}\"\n",
        "    )\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    x = self.reshape(x)\n",
        "    for conv, layernorm in zip(self.convs, self.layernorms):\n",
        "            x = conv(x)\n",
        "            x = layernorm(x)\n",
        "    x = self.output_conv(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y5YW1WdFQHR"
      },
      "source": [
        "class VAE(Model):\n",
        "  def __init__(self,\n",
        "               inp_shape: List[int],\n",
        "               conv_filters: List[int],\n",
        "               conv_kernels: List[int],\n",
        "               conv_strides: List[int],\n",
        "               latent_space_dim: int,\n",
        "               recon_loss_weight: int,\n",
        "               **kwargs):\n",
        "    super(VAE, self).__init__(**kwargs)\n",
        "    self.inp_shape = inp_shape\n",
        "    self.recon_loss_weight = recon_loss_weight \n",
        "    self._shape_before_bottleneck = None\n",
        "    self.latent_space_dim = latent_space_dim\n",
        "    self._reduce_axis = list(range(1, len(inp_shpe)+1))\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "        inp_shape=inp_shape,\n",
        "        conv_filters=conv_filters,\n",
        "        conv_kernels=conv_kernels,\n",
        "        conv_strides=conv_strides,\n",
        "        latent_space_dim=latent_space_dim\n",
        "    )\n",
        "    \n",
        "    self.decoder = Decoder(\n",
        "        shape_before_bottleneck=self.encoder._shape_before_bottleneck,\n",
        "        conv_filters = conv_filters[::-1],\n",
        "        conv_kernels = conv_kernels[::-1],\n",
        "        conv_strides=conv_strides[::-1],\n",
        "        out_channel=inp_shape[-1]\n",
        "    )\n",
        "\n",
        "  def _calculate_kl_loss(self, mu, log_var):\n",
        "    kl_loss = -0.5 * tf.reduce_sum(1 + log_var -tf.square(mu) - tf.exp(log_var), axis=1)\n",
        "    return kl_loss\n",
        "\n",
        "  def _calculate_recon_loss(self, x, x_prime):\n",
        "    recon_loss = tf.reduce_mean(tf.square(x - x_prime), axis=self._reduce_axis)\n",
        "    return self.recon_loss_weight * recon_loss\n",
        "  \n",
        "  def _compute_loss(self, x, x_prime, mu, log_var):\n",
        "    recon_loss =  self._calculate_recon_loss(x, x_prime)\n",
        "    kl_loss =  self._calculate_kl_loss(mu, log_var)\n",
        "    loss =  recon_loss  + kl_loss\n",
        "    self.add_loss(tf.add_n([loss]))\n",
        "    self.add_metric(tf.add_n([recon_loss / self.recon_loss_weight]), name=\"recon_loss\")\n",
        "    self.add_metric(tf.add_n([kl_loss]), name=\"kl_loss\")\n",
        "\n",
        "  def call(self, x):\n",
        "    z, (mu, log_var) = self.encoder(x)\n",
        "    x_prime = self.decoder(z)\n",
        "    self._compute_loss(x, x_prime, mu, log_var)\n",
        "    return z, x_prime\n",
        "\n",
        "  def full_summary(self):\n",
        "    self.encoder.summary()\n",
        "    self.decoder.summary()\n",
        "    self.summary()\n",
        "\n",
        "  def sample(self, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal([1, self.latent_space_dim])\n",
        "      return self.decoder(eps)\n",
        "    else:\n",
        "      print(f\"sample epsilon: {eps}\")\n",
        "      return self.decoder(eps)\n",
        "\n",
        "  def reconstruct(self, images):\n",
        "    latent_representations = self.encoder.predict(images)\n",
        "    reconstructed_images = self.decoder.predict(latent_representations)\n",
        "    return reconstructed_images, latent_representations\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00E6lTB9HMmr"
      },
      "source": [
        "# Training Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1LdcI0hGilJ"
      },
      "source": [
        "logdir = os.path.join(\"tensorboaed_logs/VAE_layernorms_new_arch\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03SHwcnwH2aF"
      },
      "source": [
        "def parse_function(example_proto):\n",
        "    features = {\n",
        "        'frequency': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'time': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'spectrograms': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True)\n",
        "    }\n",
        "    parsed_features = tf.io.parse_single_example(example_proto, features)\n",
        "\n",
        "    spectrogram = tf.reshape(parsed_features[\"spectrograms\"],\n",
        "                        [parsed_features[\"frequency\"], parsed_features['time']])\n",
        "\n",
        "    spectrogram = tf.transpose(spectrogram)\n",
        "    return spectrogram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX3aj4ZhITg7"
      },
      "source": [
        "def get_train_data(tfrec_path):\n",
        "    train_data = tf.data.TFRecordDataset(tfrec_path)\\\n",
        "        .shuffle(300)\\\n",
        "        .map(parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
        "        .batch(16)\\\n",
        "        .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "        \n",
        "\n",
        "    return train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI_wZTGBIFbR"
      },
      "source": [
        "tfrec_path = \"tfrec/train_data_VAE_1296_04_06_2021.tfrecord\"\n",
        "train = get_train_data(tfrec_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNLk50wtMNQb"
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQEsAMCsKHt5"
      },
      "source": [
        "vae = VAE(inp_shape=[1296, 256], \n",
        "          conv_filters=[256, 512, 512, 1024],\n",
        "          conv_kernels=[5, 5, 5, 5],\n",
        "          conv_strides=[3, 3, 3, 3],\n",
        "          latent_space_dim=1024,\n",
        "          recon_loss_weight=1000000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea1KEywlKSJi"
      },
      "source": [
        "_ = vae(Input(shape=[1296, 256]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GIU8AOwxRht"
      },
      "source": [
        "vae.compile(Adam(learning_rate=1e-4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwthi-9XdZhU",
        "outputId": "9cf97096-d2d5-43b1-d792-ca24873ec303"
      },
      "source": [
        "vae.full_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_conv_layer_0 (Conv1D multiple                  327936    \n",
            "_________________________________________________________________\n",
            "encoder_conv_layer_1 (Conv1D multiple                  655872    \n",
            "_________________________________________________________________\n",
            "encoder_conv_layer_2 (Conv1D multiple                  1311232   \n",
            "_________________________________________________________________\n",
            "encoder_conv_layer_3 (Conv1D multiple                  2622464   \n",
            "_________________________________________________________________\n",
            "encoder_ln_0 (LayerNormaliza multiple                  512       \n",
            "_________________________________________________________________\n",
            "encoder_ln_1 (LayerNormaliza multiple                  1024      \n",
            "_________________________________________________________________\n",
            "encoder_ln_2 (LayerNormaliza multiple                  1024      \n",
            "_________________________________________________________________\n",
            "encoder_ln_3 (LayerNormaliza multiple                  2048      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "mu (Dense)                   multiple                  16778240  \n",
            "_________________________________________________________________\n",
            "log_variance (Dense)         multiple                  16778240  \n",
            "=================================================================\n",
            "Total params: 38,478,592\n",
            "Trainable params: 38,478,592\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_dense (Dense)        multiple                  16793600  \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer multiple                  2621952   \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer multiple                  1311232   \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer multiple                  655616    \n",
            "_________________________________________________________________\n",
            "decoder_ln_0 (LayerNormaliza multiple                  1024      \n",
            "_________________________________________________________________\n",
            "decoder_ln_1 (LayerNormaliza multiple                  1024      \n",
            "_________________________________________________________________\n",
            "decoder_ln_2 (LayerNormaliza multiple                  512       \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer multiple                  327936    \n",
            "=================================================================\n",
            "Total params: 21,712,896\n",
            "Trainable params: 21,712,896\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"vae\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder (Encoder)            multiple                  38478592  \n",
            "_________________________________________________________________\n",
            "decoder (Decoder)            multiple                  21712896  \n",
            "=================================================================\n",
            "Total params: 60,191,488\n",
            "Trainable params: 60,191,488\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6APORwyJs3n"
      },
      "source": [
        "checkpoint_file_path = \"model/checkpoint_VAE_256_512_512_1024_rc1000000_layernorm/weight_improvement_{epoch:02d}-{loss:.4f}\"\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_file_path,\n",
        "    monitor=\"loss\",\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode=\"min\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}